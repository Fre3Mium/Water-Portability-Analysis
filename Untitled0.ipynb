{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVUTNQij35vfBHxDQC3zNQ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Libraries**"
      ],
      "metadata": {
        "id": "QcgF4JsXl-_i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-1T5tnXPkS_j"
      },
      "outputs": [],
      "source": [
        "# 1. to handle the data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# to visualize the data\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "# To preprocess the data\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder,OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "# import iterative imputer\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "# machine learning\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "#for classification tasks\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, RandomForestRegressor\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "# pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "# metrics\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_absolute_error,mean_squared_error,r2_score\n",
        "\n",
        "# ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the **Data**\n"
      ],
      "metadata": {
        "id": "-tn3-qPWmMFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read data\n",
        "df = pd.read_csv(\"water_potability.csv\")\n",
        "# print first 5 rows\n",
        "df.head()"
      ],
      "metadata": {
        "id": "p02SmIs5mVwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EDA**"
      ],
      "metadata": {
        "id": "BFD-GM7jmzmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print descriptive statistics\n",
        "df.describe().style.background_gradient(cmap='coolwarm')"
      ],
      "metadata": {
        "id": "I19XXw7hm5UA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Print Info**"
      ],
      "metadata": {
        "id": "CLs6i_ddoH5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print info\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "Isy-3aO7oJsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Target Distribution**"
      ],
      "metadata": {
        "id": "eVn4iMC_oZ-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Target distribution\n",
        "plt.figure(figsize=(6,6))\n",
        "# Pie plot\n",
        "df['Potability'].value_counts().plot.pie(explode=[0.1,0.1],\n",
        "                    autopct='%1.1f%%', shadow=True,\n",
        "                    textprops={'fontsize':16}).set_title(\"Target distribution\");"
      ],
      "metadata": {
        "id": "RWrLqFfQogXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Histogram**"
      ],
      "metadata": {
        "id": "3D9ak86IpOpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df is your DataFrame\n",
        "variables = ['ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity',\n",
        "             'Organic_carbon', 'Trihalomethanes', 'Turbidity']\n",
        "\n",
        "# Set up the figure and axes\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15))\n",
        "\n",
        "# Flatten the axes for easy iteration\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Loop through each variable and create histograms\n",
        "for i, var in enumerate(variables):\n",
        "    ax = axes[i]\n",
        "    sns.histplot(df[var], kde=True, ax=ax)\n",
        "    ax.axvline(df[var].mean(), color='red', linestyle='--', label='Mean')\n",
        "    ax.axvline(df[var].median(), color='blue', linestyle='--', label='Median')\n",
        "\n",
        "    # Annotate plot with mean and median\n",
        "    ax.annotate(f'Mean: {df[var].mean():.2f}\\nMedian: {df[var].median():.2f}',\n",
        "                xy=(0.05, 0.95), xycoords='axes fraction', ha='left', va='top')\n",
        "\n",
        "    ax.set_title(f'Histogram with KDE for {var}')\n",
        "    ax.set_xlabel(var)\n",
        "    ax.legend()\n",
        "\n",
        "# Adjust layout and display\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LpTpIQazpSJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Corelation**"
      ],
      "metadata": {
        "id": "V-JdQ9XopjdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now check the correlation of all columns\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.heatmap(df.corr(),annot=True,cmap='coolwarm',linewidths=0.4);"
      ],
      "metadata": {
        "id": "pnS9xpm6plT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parallel coordinates**"
      ],
      "metadata": {
        "id": "CPswE87npmXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "fig = px.parallel_coordinates(df, color=\"Potability\", color_continuous_scale=['#8DBAFF', '#890D0D'],\n",
        "                               color_continuous_midpoint=2, height=800, width=1200)\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "NemAp_H0pvBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Missing values**"
      ],
      "metadata": {
        "id": "YVbVzUl3qFF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print missing values\n",
        "df.isnull().sum().sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "RZMIFvowqGqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imputing Missing Values**"
      ],
      "metadata": {
        "id": "NtBLCTQHqQJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# missing data columns\n",
        "missing_data_cols =df.columns[df.isnull().any()]\n",
        "\n",
        "numeric_cols = ['Sulfate','ph','Trihalomethanes']\n",
        "# Define numeric_cols,\n",
        "\n",
        "def impute_continuous_missing_data(passed_col):\n",
        "    df_null = df[df[passed_col].isnull()]\n",
        "    df_not_null = df[df[passed_col].notnull()]\n",
        "\n",
        "    X = df_not_null.drop(passed_col, axis=1)\n",
        "    y = df_not_null[passed_col]\n",
        "\n",
        "    other_missing_cols = [col for col in missing_data_cols if col != passed_col]\n",
        "\n",
        "    iterative_imputer = IterativeImputer(estimator=RandomForestRegressor(random_state=42), add_indicator=True)\n",
        "\n",
        "    for col in other_missing_cols:\n",
        "        if X[col].isnull().sum() > 0:\n",
        "            col_with_missing_values = X[col].values.reshape(-1, 1)\n",
        "            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)\n",
        "            X[col] = imputed_values[:, 0]\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    rf_regressor = RandomForestRegressor()\n",
        "    rf_regressor.fit(X_train, y_train)\n",
        "    y_pred = rf_regressor.predict(X_test)\n",
        "\n",
        "    print(\"MAE =\", mean_absolute_error(y_test, y_pred))\n",
        "    print(\"RMSE =\", mean_squared_error(y_test, y_pred, squared=False))\n",
        "    print(\"R2 =\", r2_score(y_test, y_pred))\n",
        "\n",
        "    X = df_null.drop(passed_col, axis=1)\n",
        "\n",
        "    for col in other_missing_cols:\n",
        "        if X[col].isnull().sum() > 0:\n",
        "            col_with_missing_values = X[col].values.reshape(-1, 1)\n",
        "            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)\n",
        "            X[col] = imputed_values[:, 0]\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    if len(df_null) > 0:\n",
        "        df_null[passed_col] = rf_regressor.predict(X)\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    df_combined = pd.concat([df_not_null, df_null])\n",
        "    # Check if all missing values have been imputed\n",
        "    if df_null[passed_col].isnull().sum() == 0:\n",
        "        print(\"All missing values in column\", passed_col, \"imputed successfully.\")\n",
        "\n",
        "    return df_combined[passed_col]\n",
        "\n",
        "# Impute missing values for numeric columns\n",
        "for col in numeric_cols:\n",
        "    print(\"Missing Values\", col, \":\", str(round((df[col].isnull().sum() / len(df)) * 100, 2))+\"%\")\n",
        "    if col in numeric_cols:\n",
        "        df[col] = impute_continuous_missing_data(col)"
      ],
      "metadata": {
        "id": "H54Hn_csqSEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Machine Learning Model** Using Random Forest"
      ],
      "metadata": {
        "id": "XLj6GcZkqkgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split data into X and y\n",
        "X = df.drop('Potability', axis=1)\n",
        "y = df['Potability']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "M5LFIMBmqnWq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, accuracy_score\n",
        "\n",
        "# Define the pipeline with a scaler and the random forest classifier\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', MinMaxScaler()),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Define the hyperparameters for grid search\n",
        "params = {\n",
        "    'classifier__n_estimators': [100, 200, 300],\n",
        "    'classifier__max_depth': [10, 20, 30]\n",
        "}\n",
        "\n",
        "# Perform grid search using the pipeline and parameters\n",
        "clf = GridSearchCV(pipeline, params, cv=5)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model and its parameters\n",
        "best_model = clf.best_estimator_\n",
        "best_model.fit(X_train, y_train)\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_best)\n",
        "f1 = f1_score(y_test, y_pred_best)\n",
        "precision = precision_score(y_test, y_pred_best)\n",
        "\n",
        "# Print the results\n",
        "print(\"Best Random Forest Model:\")\n",
        "print(\"Test Accuracy:\", accuracy_best)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Precision Score:\", precision)\n",
        "print(\"Confusion Matrix:\",conf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJOOfXquqp1-",
        "outputId": "f36b9e12-95a9-4829-f233-4bd15c76f868"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Random Forest Model:\n",
            "Test Accuracy: 0.7073170731707317\n",
            "F1 Score: 0.5102040816326531\n",
            "Precision Score: 0.6756756756756757\n",
            "Confusion Matrix: [[364  48]\n",
            " [144 100]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.Potability.value_counts()\n"
      ],
      "metadata": {
        "id": "CQb7KfN1sNyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Integrating Flask**"
      ],
      "metadata": {
        "id": "OMG1BvyTsxwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from flask import Flask, render_template, request\n",
        "import joblib\n",
        "\n",
        "# Load the trained ML model\n",
        "model = joblib.load('your_model.pkl')\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Define route for home page\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return render_template('index.html')\n",
        "\n",
        "# Define route for prediction\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    # Get user input from the form\n",
        "    ph = float(request.form['ph'])\n",
        "    hardness = float(request.form['hardness'])\n",
        "    # Add more variables as needed\n",
        "\n",
        "    # Make prediction using the ML model\n",
        "    prediction = model.predict([[ph, hardness]])[0]\n",
        "\n",
        "    # Determine the result message\n",
        "    if prediction == 1:\n",
        "        result = 'Potable'\n",
        "    else:\n",
        "        result = 'Non-Potable'\n",
        "\n",
        "    return render_template('result.html', result=result)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "GjlFPopEs0_-",
        "outputId": "8f935b5b-39bc-4199-927a-3eb55552391e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'your_model.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-09b05df4b2d7>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load the trained ML model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'your_model.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Initialize Flask app\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_model.pkl'"
          ]
        }
      ]
    }
  ]
}